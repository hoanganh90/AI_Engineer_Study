{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create meeting minutes from an Audio file\n",
        "\n",
        "I downloaded some Denver City Council meeting minutes and selected a portion of the meeting for us to transcribe. You can download it here:  \n",
        "https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing\n",
        "\n",
        "If you'd rather work with the original data, the HuggingFace dataset is [here](https://huggingface.co/datasets/huuuyeah/meetingbank) and the audio can be downloaded [here](https://huggingface.co/datasets/huuuyeah/MeetingBank_Audio/tree/main).\n",
        "\n",
        "The goal of this product is to use the Audio to generate meeting minutes, including actions.\n",
        "\n",
        "For this project, you can either use the Denver meeting minutes, or you can record something of your own!\n"
      ],
      "metadata": {
        "id": "It89APiAtTUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Again - please note: 2 important pro-tips for using Colab:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "The top of every colab has some pip installs. You may receive errors from pip when you run this, such as:\n",
        "\n",
        "> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
        "\n",
        "These pip compatibility errors can be safely ignored; and while it's tempting to try to fix them by changing version numbers, that will actually introduce real problems!\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "In the middle of running a Colab, you might get an error like this:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "This is a super-misleading error message! Please don't try changing versions of packages...\n",
        "\n",
        "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
        "\n",
        "1. Kernel menu >> Disconnect and delete runtime\n",
        "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
        "3. Connect to a new T4 using the button at the top right\n",
        "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
        "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
        "\n",
        "And all should work great - otherwise, ask me!"
      ],
      "metadata": {
        "id": "sJPSCwPX3MOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai"
      ],
      "metadata": {
        "id": "f2vvgnFpHpID"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqruQpl7lfLV",
        "outputId": "e4d13852-53d3-4959-f396-c1d0b6458a07"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "from google import genai\n",
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "AUDIO_MODEL = \"whisper-1\"\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New capability - connect this Colab to my Google Drive\n",
        "# See immediately below this for instructions to obtain denver_extract.mp3\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "audio_filename = \"/content/drive/My Drive/llms/denver_extract.mp3\""
      ],
      "metadata": {
        "id": "Es9GkQ0FGCMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0c1b64-3c08-49da-8552-51565adb23e6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download denver_extract.mp3\n",
        "\n",
        "You can either use the same file as me, the extract from Denver city council minutes, or you can try your own..\n",
        "\n",
        "If you want to use the same as me, then please download my extract here, and put this on your Google Drive:  \n",
        "https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "HTl3mcjyzIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "xYW8kQYtF-3L"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI using Secrets in Colab\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "qP6OB2OeGC2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "6f913e7c-bc57-4658-9e89-396bf26a4e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret OPENAI_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3978465297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sign in to OpenAI using Secrets in Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mopenai_api_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPENAI_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenai_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret OPENAI_API_KEY does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")\n",
        "client = genai.Client(api_key=google_api_key)\n",
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite-preview-06-17\",\"gemini-2.0-flash\",\"gemini-2.5-flash\",\"gemini-2.5-pro\"] {\"allow-input\":true, isTemplate: true}\n",
        "audio_file = client.files.upload(file=\"/content/drive/My Drive/llms/denver_extract.mp3\")\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL_ID,\n",
        "  contents=[\n",
        "    'Listen carefully to the following audio file. Provide a transcription.',\n",
        "    audio_file,\n",
        "  ]\n",
        ")\n",
        "transcription = response.text\n",
        "print(transcription)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFtQcWa899w8",
        "outputId": "7b4723e4-5f7b-4b38-c810-62d8134e0697"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API Key exists and begins AIzaSyCQ\n",
            "- and kind of the confluence of this whole idea of the confluence week, the merging of two rivers. And uh as we've kind of seen recently in politics and in the world, there's a lot of um situations where water is very important right now and it's a very big issue. So that is the reason that the back of the logo is considered water. So, let you see the creation of the logo here. [papers rustling] And then yeah, so that basically um kind of sums up the reason behind the logo and the um all the meanings behind the symbolism. And uh I'll you'll hear a little bit more about our Confluence Week is basically highlighting all of these indigenous events and things that are happening around Denver so that we can kind of bring more people together and kind of share this whole idea of Indigenous People's Day. So, thank you.\n",
            "- Thank you so much and thanks for your leadership.\n",
            "- All right.\n",
            "- Welcome to the Denver City Council meeting of Monday, October 9th. Please rise with the Pledge of Allegiance by Councilman Lopez.\n",
            "- I pledge allegiance to the flag of the United States of America, and to the Republic for which it stands, one nation, under God, indivisible, with liberty and justice for all.\n",
            "- All right, thank you, Councilman Lopez. Madam Secretary, roll call.\n",
            "- Black?\n",
            "- Clerk?\n",
            "- Here.\n",
            "- Espinosa?\n",
            "- Here.\n",
            "- Flynn?\n",
            "- Gilmore?\n",
            "- Here.\n",
            "- [unintelligible]\n",
            "- Here.\n",
            "- Cashman?\n",
            "- Here.\n",
            "- Kenneich?\n",
            "- Here.\n",
            "- Lopez?\n",
            "- Here.\n",
            "- New?\n",
            "- Here.\n",
            "- Ortega?\n",
            "- Here.\n",
            "- Sasman?\n",
            "- Here.\n",
            "- Miss President?\n",
            "- Here.\n",
            "- Eleven members present.\n",
            "- Eleven members present, we do have a quorum. Approval of the minutes. Are there any corrections to the minutes of October 2nd? Seeing none, minutes of October 2nd stand approved. Council announcements. Are there any announcements by members of Council?\n",
            "- Councilman Clark?\n",
            "- Thank you, Mr. President. Um, I just want to invite everyone down to the first ever Halloween parade on Broadway and Lucky District 7. It will happen on Saturday, October 21st, at 6:00 p.m. It will move um along Broadway from Third to Alameda. It's going to be a fun, family-friendly event. Everyone's invited to come down, wear a costume, um there will be candy for the kids and um there are Tiki zombies and uh 29 hearses and all kinds of fun and funky stuff on uh the fun and funky part of Broadway. So, please join us October 21st at 6:00 for the Broadway Halloween parade. Thank you, Mr. President.\n",
            "- All right, thank you, Councilman Clark. I will be there. All right. Uh, presentations. Madam Secretary, do we have any presentations?\n",
            "- None, Mr. President.\n",
            "- Communications, do we have any communications?\n",
            "- None, Mr. President.\n",
            "- We do have one proclamation this evening. Uh Proclamation 1127, an observance of uh the annual Indigenous Peoples' uh Day in the City and County of Denver. Councilman Lopez, would you please read it?\n",
            "- Thank you, Mr. President, with pride. Uh, Proclamation Number 17, well, let me just say this differently. Proclamation Number 1127, Series of 2017, an observance of the Second Annual Indigenous Peoples' Day in the City and County of Denver. Whereas the Council of the City and County of Denver recognizes that the indigenous peoples have lived and flourished on the lands known as the Americas since time immemorial, and that Denver and the surrounding communities are built upon the ancestral homelands of numerous indigenous tribes, which include the Southern Ute, the Ute Mountain Ute Tribes of Colorado, and whereas the tribal homelands and seasonal encampments of the Arapaho and Cheyenne people along the banks of the Cherry Creek and South Platte River Confluence gave bearing to the future settlements that would become the birthplace of the Mile High City. And whereas, Colorado encompasses the ancestral homelands of 48 tribes and the City and County of Denver and surrounding communities are home to the descendants of approximately 100 tribal nations. And whereas, on October 3rd, 2016, the City and County of Denver unanimously passed Council Bill 801, Series of 2016, officially designating the second Monday of October of each year as Indigenous Peoples' Day in Denver, Colorado. And whereas, the Council of the City and County of Denver continues to recognize and value the vast contributions made to community, made to the community through Indigenous Peoples' knowledge, science, philosophy, art, and culture. And through these contributions, the City of Denver has developed and thrived. Whereas, the indigenous community, especially youth, have made great efforts this year to draw attention to the contributions of indigenous people including Confluence Week. Drawing record attendance to a National Indigenous Youth Leadership Conference. Leading conversations on inclusion with their peers, and supporting increased indigenous youth participation in science and engineering. Now, therefore, be it proclaimed by the Council of the City and County of Denver, Section 1, that the Council of the City and County of Denver celebrates and honors the cultural and foundational contributions of indigenous people to our history, our past, our present, and future. And continues to promote the education of the Denver, of the Denver community about these historical and contemporary contributions of indigenous people. Section 2, that the City and County of Denver, Colorado does hereby observe October 9th, 2017 as Indigenous Peoples' Day. Section 3, that the Clerk of the City and County of Denver shall attest and affix the seal of the City and County of Denver to this proclamation, and that a copy be trans, excuse me, to the Denver American Indian Commission, the City and County of Denver School District Number 1, and the Colorado Commission on Indian Affairs.\n",
            "- Thank you, Councilman Lopez. Your motion to adopt.\n",
            "- Mr. President, I moved that proclamation number 1127 series of 2017 be adopted.\n",
            "- All right, it has been moved and seconded. Comments by members of Council, Councilman Lopez.\n",
            "- Thank you, Mr. President. Um, it gives me a lot of pleasure and and and pride to read this proclamation officially for this, for the third time, but as Indigenous Peoples' Day in Denver, officially for the second time. Um, it is, uh, it's always awesome to be able to see, um, not just this proclamation come through, come by my desk, but to see so many different people from our community in our in our council chambers. Um, it was a very beautiful piece of artwork that you presented to us earlier, and it is exactly the spirit that we um, drafted this proclamation, and this actual the the ordinance that created Indigenous Peoples' Day when we sat down and wrote it and as a community, we couldn't think of anything else to begin except for the confluence of the two rivers. And those confluence of the two rivers created such a great city. We live in such an amazing city and we we're all proud of it and sometimes we um, and and a lot of people from all over the country are, all over the world are proud of it, and sometimes a little too proud of it, that's just time to go back home. Um, but, um, I'm I'm kidding when I say that. Um, but the the really nice thing about this is that, um, we are celebrating Indigenous Peoples' Day out of pride for who we are and who we are as a city and and the contributions of indigenous people to the city, not out of spite, not out of, um, a replacement of one culture over the other, or or out of, um, contempt, or, um, or disrespect. You know, I I think of uh, a quote that César Chávez uh made very um, very popular and, uh, it's stuck with me for a very long time and and I and I, any if anytime I have the opportunity, I to speak in front of children and especially children in our community that, you know, they they um, often second guess themselves and where they're coming from, who they are, and um, and I always say that, you know, it's it's very important to be proud of where we are from. And the quote that I use from César Chávez is, um, you know, pride in one's own culture does not require contempt or disrespect of another. Right? And that's very important. It's very important for us to recognize that. No matter who we are, where we come from in this society, that your pride in your own culture doesn't require, should not require the contempt or disrespect of another. And man, what a year to be for that to to sit on our shoulders for a while for us to think about. Right? And so, I wanted to um, uh, just to to thank you all, to thank the commission. Um, there's going to be a couple individuals that are going to come speak. Thank you for your art, your lovely artwork for us to see, uh, what's in your heart and what now has become a, probably is going to be a very important symbol uh for the community. And also just for the work, the the daily work, every single day. We still have a lot of brothers and sisters whose ancestors once lived in these lands freely, now stand on street corners, right? In poverty. Um, without access to services, right? Um, without access to sobriety or even housing or jobs. And what a, what a what a cruel way to pay back a culture that has paved the way for the city to be built upon its shores, right? So, um, we have a lot of work to do. And these kind of proclamations and this day is not a day off, it's a day on in Denver. Uh, right? And and and addressing those those those critical issues. So, um, I I know that my colleagues are very supportive. I'm going to ask you to support this proclamation as I know you always have done in the past. I'm very proud of today. Oh, and we made Time Magazine and Newsweek once again today as being a leader in terms of the cities that are are celebrating Indigenous Peoples' Day. I wanted to make a point out of that.\n",
            "- Thank you, uh, Councilman Lopez, and thank you for sponsoring this. Councilwoman Ortega?\n",
            "- Mr. President, I want to ask that my name be added. I don't think I could, um, add much more to what Councilman Lopez has, um, shared with us. Um, I want to thank him for bringing this forward. And, um, really just appreciate all the contributions that our, um, Native American community has contributed to this great city and great state. Um, I worked in the Lieutenant Governor's office when the Commission on Indian Affairs was created, and had the benefit of being able to go down to, um, the Four Corners for a peace treaty signing ceremony between the Utes and the Comanches that had been sort of at at odds with each other for about a hundred years. And just being able to participate in that powwow was was pretty awesome. So, um, and for those of you who continue to participate in the annual powwow, it's it's such a great opportunity for everybody else to enjoy so many of the contributions of the culture. I mean, to see that the dance continues to be carried on as well as as the native language from generation to generation is just so incredible because in so many cultures, you know, people have come here and, um, assimilated to the the, you know, the norms here and they lose their language and and and lose a lot of the culture. And in the native community, that that hasn't happened. That has that, you know, um, commitment to just passing that on from generation to generation is is so important. And so, I'm I'm happy to be a co-sponsor of this tonight. Thank you.\n",
            "- All right, thank you, Councilwoman Ortega. Councilwoman Kenneich.\n",
            "- Thank you very much, and I also want to thank my colleague for bringing this forward. Um, and I I I just wanted to to say a word to the artist about how beautiful and moving I thought this logo was and your description of it. Um, and I think one of the things, um, that is clear is, you know, the words sometimes don't convey the power of imagery or music or the other pieces that make up culture. And so, I think the art is so important. And when you talked about water, I was also thinking about land. And I guess I just wanted to say thank you. Um, many of the Native American peoples of Colorado have been at the forefront, or actually nationally, of defending some of the the, um, public lands that have been protected over the last few years that are under attack right now. And they're places that you all, um, the communities have fought to protect, but that everyone gets to enjoy. And so, I just think that it's an example of where cultural preservation intersects with environmental protection, with, you know, recreation and all of the other ways that that public lands are so important. And so, I think, um, I just wanted to say thank you for that because I think we have some very sacred places in our country that are at risk right now. And, um, so, as we celebrate, I appreciate that there's still a piece of resistance in here. And I think that, um, I just want to mention a solidarity, I want to mention a feeling of solidarity with that resistance. So, thank you, and, uh, happy Confluence Week.\n",
            "- Thank you, uh, Councilwoman Kenneich, uh, and seeing no other comments, I'll just say a couple. And, um, in a time of such divisive, um, ugliness and just despicable behavior from our leadership, um, the reason I'm so supportive of Indigenous Peoples' Day is because it means inclusivity. It means respecting all, respecting those who have been silenced, um, uh, on purpose for a long time and whose history has not been told. And so, we celebrate inclusivity in the face of such, um, such evil times, honestly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Whisper OpenAI model to convert the Audio to Text\n",
        "# If you'd prefer to use an Open Source model, class student Youssef has contributed an open source version\n",
        "# which I've added to the bottom of this colab\n",
        "\n",
        "audio_file = open(audio_filename, \"rb\")\n",
        "transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "GMShdVGlGGr4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "dc3dd421-ac88-46ed-cb60-f766fd747a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'openai' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-745770441.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtranscription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUDIO_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'openai' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f9SyOJVFMpg",
        "outputId": "7aaeaa63-c7c2-45f3-afb9-1624df1ee606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded audio file into bytes. Size: 13.73 MB\n",
            "\n",
            "Attempting to transcribe using INLINE audio data...\n",
            "An error occurred during Gemini API call: name 'types' is not defined\n",
            "Please check your API key, network connection, and model availability.\n",
            "If the error is about request payload size, ensure you are using the Files API for large files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
        "user_prompt = f\"Below is an extract transcript of a Denver council meeting. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "piEMmcSfMH-O"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "UcRKUgcxMew6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "HvXc0tZfnD2Y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "streamer = TextStreamer(tokenizer)\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
        "outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "GMNhfxOomvmj",
        "outputId": "e25b9918-8a3d-4156-8829-a5b48d333135"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers.models.bitnet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-65-2305045763.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstreamer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextStreamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLAMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_local_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_remote_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n\u001b[0;32m--> 547\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;31m# Set the adapter kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"adapter_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping_keys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.bitnet'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "id": "102tdU_3Peam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ba2a1312-f6f5-45d2-dc47-d85d8183b61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'outputs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-38-1889815539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response))"
      ],
      "metadata": {
        "id": "KlomN6CwMdoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student contribution\n",
        "\n",
        "Student Emad S. has made this powerful variation that uses `TextIteratorStreamer` to stream back results into a Gradio UI, and takes advantage of background threads for performance! I'm sharing it here if you'd like to take a look at some very interesting work. Thank you, Emad!\n",
        "\n",
        "https://colab.research.google.com/drive/1Ja5zyniyJo5y8s1LKeCTSkB2xyDPOt6D"
      ],
      "metadata": {
        "id": "kuxYecT2QDQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative implementation\n",
        "\n",
        "Class student Youssef has contributed this variation in which we use an open-source model to transcribe the meeting Audio.\n",
        "\n",
        "Thank you Youssef!"
      ],
      "metadata": {
        "id": "AU3uAEyU3a-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_MODEL = \"openai/whisper-medium\"\n",
        "speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)\n",
        "speech_model.to('cuda')\n",
        "processor = AutoProcessor.from_pretrained(AUDIO_MODEL)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=speech_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch.float16,\n",
        "    device='cuda',\n",
        ")"
      ],
      "metadata": {
        "id": "HdQnWEzW3lzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Whisper OpenAI model to convert the Audio to Text\n",
        "result = pipe(audio_filename)"
      ],
      "metadata": {
        "id": "nrQjKtD53omJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = result[\"text\"]\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "G_XSljOY3tDf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}