{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfa9ae6-69fe-444a-b994-8c4c5970a7ec",
   "metadata": {},
   "source": [
    "# Project - Airline AI Assistant\n",
    "\n",
    "We'll now bring together what we've learned to make an AI Customer Support assistant for an Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b50bbe2-c0b1-49c3-9a5c-1ba7efa2bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "747e8786-9da8-4342-b6c9-f5f69c2e22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AIzaSyCQ\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "\n",
    "# Gemini API setup\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n",
    "\n",
    "\n",
    "# As an alternative, if you'd like to use Ollama instead of OpenAI\n",
    "# Check that Ollama is running for you locally (see week1/day2 exercise) then uncomment these next 2 lines\n",
    "# MODEL = \"llama3.2\"\n",
    "# openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a521d84-d07c-49ab-a0df-d6451499ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61a2a15d-b559-4844-b377-6bd5cb4949f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function looks rather simpler than the one from my video, because we're taking advantage of the latest Gradio updates\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def transform_message_for_gemini(message):\n",
    "    return {\n",
    "        \"role\": \"user\" if message[\"role\"] == \"user\" else \"model\", # Map your \"assistant\" to \"model\"\n",
    "        \"parts\": [\n",
    "            {\n",
    "                \"text\": message[\"content\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def append_history_to_chat_data(existing_chat_history, history_response):\n",
    "    for message in history_response:\n",
    "        transformed_message = transform_message_for_gemini(message)\n",
    "        existing_chat_history.append(transformed_message)\n",
    "    return existing_chat_history\n",
    "\n",
    "def chat_Gemini(message, history):\n",
    "   \n",
    "   # 1. Initialize your chat_history in the Gemini API format (empty for a new chat)\n",
    "    chat_history = []\n",
    "    updated_chat_history = append_history_to_chat_data(chat_history, history)\n",
    "\n",
    "    # 2. Append the new user message to the chat history\n",
    "    updated_chat_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\n",
    "            {\n",
    "                \"text\": message\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    #1 Create an instance of the GenerativeModel class\n",
    "    model_instance = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash\",\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    stream = model_instance.generate_content(contents=updated_chat_history, stream=True)\n",
    "    #3 Iterate over the stream to get the response\n",
    "    full_response_text = \"\"\n",
    "\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end='') \n",
    "            full_response_text += chunk.text\n",
    "            yield full_response_text\n",
    "\n",
    "#gr.ChatInterface(fn=chat, type=\"messages\").launch()\n",
    "gr.ChatInterface(fn=chat_Gemini, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bedabf-a0a7-4985-ad8e-07ed6a55a3a4",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Tools are an incredibly powerful feature provided by the frontier LLMs.\n",
    "\n",
    "With tools, you can write a function, and have the LLM call that function as part of its response.\n",
    "\n",
    "Sounds almost spooky.. we're giving it the power to run code on our machine?\n",
    "\n",
    "Well, kinda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0696acb1-0b05-4dc2-80d5-771be04f1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80ca4e09-6287-4d3f-997d-fa6afbcf6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool get_ticket_price called for Berlin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'$499'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ticket_price(\"Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4afceded-7178-4c05-8fa6-9f2085e6a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\",\n",
    "    \"description\": \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# This is the function the Gemini model will be able to call.\n",
    "def price_function_2(item: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the price of a given item.\n",
    "    Args:\n",
    "        item: The name of the item for which to find the price.\n",
    "    \"\"\"\n",
    "    print(f\"Tool 'price_function' called with item: {item}\")\n",
    "    if \"shirt\" in item.lower():\n",
    "        return \"The price of a shirt is $25.\"\n",
    "    elif \"jeans\" in item.lower():\n",
    "        return \"The price of jeans is $50.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't have a price for {item}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdca8679-935f-4e7f-97e6-e71a4d4f228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And this is included in a list of tools:\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]\n",
    "\n",
    "# Tools for Gemini\n",
    "tools_For_Gemini = [price_function_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3554f-b4e3-4ce7-af6f-68faa6dd2340",
   "metadata": {},
   "source": [
    "## Getting OpenAI to use our Tool\n",
    "\n",
    "There's some fiddly stuff to allow OpenAI \"to call our tool\"\n",
    "\n",
    "What we actually do is give the LLM the opportunity to inform us that it wants us to run the tool.\n",
    "\n",
    "Here's how the new chat function looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b0744-9c78-408d-b9df-9f6fd9ed78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    return response, city\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response, city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# --- Helper Function for History ---\n",
    "def convert_history_to_gemini_format(history: List[Tuple[str, str]]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Converts Gradio's chat history format to Gemini's format.\n",
    "    Gradio: [(\"user message\", \"model message\"), ...]\n",
    "    Gemini: [{\"role\": \"user\", ...}, {\"role\": \"model\", ...}, ...]\n",
    "    \"\"\"\n",
    "    gemini_history = []\n",
    "    for user_msg, model_msg in history:\n",
    "        gemini_history.append({\"role\": \"user\", \"parts\": [{\"text\": user_msg}]})\n",
    "        gemini_history.append({\"role\": \"model\", \"parts\": [{\"text\": model_msg}]})\n",
    "    return gemini_history\n",
    "\n",
    "def chat_Gemini_with_Tool(message: str, history: List[Tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    Handles the chat interaction with the Gemini model, including tool calls.\n",
    "    \"\"\"\n",
    "    # 1. Initialize the Gemini model\n",
    "    # We pass the tools directly to the model during initialization.\n",
    "    model_instance = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash\",\n",
    "        system_instruction=system_message,\n",
    "        tools=tools_For_Gemini\n",
    "    )\n",
    "\n",
    "    # 2. Convert and build the chat history for the API call\n",
    "    gemini_history = convert_history_to_gemini_format(history)\n",
    "    gemini_history.append({\"role\": \"user\", \"parts\": [{\"text\": message}]})\n",
    "\n",
    "    # 3. Call the model\n",
    "    # We set stream=False because we need the full response to check for tool calls.\n",
    "    # If a tool is called, we must execute it and send the result back before\n",
    "    # we can get the final text response to show the user.\n",
    "    print(\"Sending request to Gemini...\")\n",
    "    response = model_instance.generate_content(\n",
    "        gemini_history,\n",
    "        tool_config={\"function_calling_config\": \"any\"}\n",
    "    )\n",
    "\n",
    "    # 4. Check if the model's response includes a tool call\n",
    "    candidate = response.candidates[0]\n",
    "    if candidate.finish_reason == 'TOOL_CALLS':\n",
    "        print(\"Gemini responded with a tool call.\")\n",
    "        # The model wants to call a function.\n",
    "        # For this example, we'll process the first tool call.\n",
    "        tool_call = candidate.content.parts[0].function_call\n",
    "        tool_name = tool_call.name\n",
    "\n",
    "        if tool_name == 'price_function_2':\n",
    "            # A. Extract arguments and call the actual Python function\n",
    "            args = {key: value for key, value in tool_call.args.items()}\n",
    "            tool_result = price_function_2(**args)\n",
    "\n",
    "            # B. Append the model's tool call and our tool's response to the history\n",
    "            gemini_history.append(candidate.content)  # Add model's request to history\n",
    "            gemini_history.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"parts\": [{\n",
    "                    \"function_response\": {\n",
    "                        \"name\": \"price_function_2\",\n",
    "                        \"response\": {\"result\": tool_result}\n",
    "                    }\n",
    "                }]\n",
    "            })\n",
    "\n",
    "            # C. Send the tool response back to the model to get a final answer\n",
    "            print(\"Sending tool result back to Gemini...\")\n",
    "            final_response = model_instance.generate_content(gemini_history)\n",
    "            return final_response.text\n",
    "        else:\n",
    "            # Handle cases where the model calls a function you haven't defined\n",
    "            return f\"Error: Model tried to call an unknown function: {tool_name}\"\n",
    "    else:\n",
    "        # 5. If no tool call, just return the model's text response\n",
    "        print(\"Gemini responded with text.\")\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be8a71-b19e-4c2f-80df-f59ff2661f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gr.ChatInterface(fn=chat, type=\"messages\").launch()\n",
    "gr.ChatInterface(fn=chat_Gemini_with_Tool, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de71959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradio interface...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio interface running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to Gemini...\n",
      "Gemini finished with reason: 1. Attempting to return text if available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 861, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_17780\\1398784686.py\", line 124, in chat_Gemini_with_Tool\n",
      "    return response.text if response.text else f\"Sorry, Gemini finished with reason: {candidate.finish_reason} and no text was generated.\"\n",
      "                            ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\hoang\\.conda\\envs\\llms\\Lib\\site-packages\\google\\generativeai\\types\\generation_types.py\", line 536, in text\n",
      "    raise ValueError(f\"Could not convert `part.{part_type}` to text.\")\n",
      "ValueError: Could not convert `part.function_call` to text.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from typing import List, Tuple\n",
    "\n",
    "# --- Configuration ---\n",
    "# It's best practice to use environment variables for API keys.\n",
    "# Make sure to set your GOOGLE_API_KEY in your environment.\n",
    "try:\n",
    "    genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "except KeyError:\n",
    "    print(\"ERROR: Please set the GOOGLE_API_KEY environment variable.\")\n",
    "    exit()\n",
    "\n",
    "# --- Tool Definition ---\n",
    "# This is the function the Gemini model will be able to call.\n",
    "def price_function(item: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the price of a given item.\n",
    "    Args:\n",
    "        item: The name of the item for which to find the price.\n",
    "    \"\"\"\n",
    "    print(f\"Tool 'price_function' called with item: {item}\")\n",
    "    if \"shirt\" in item.lower():\n",
    "        return \"The price of a shirt is $25.\"\n",
    "    elif \"jeans\" in item.lower():\n",
    "        return \"The price of jeans is $50.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't have a price for {item}.\"\n",
    "\n",
    "# --- Model and Chat Configuration ---\n",
    "system_message = (\n",
    "    \"You are a helpful shopping assistant. \"\n",
    "    \"You can use tools to find the price of items. \"\n",
    "    \"When a user asks for a price, use the provided tool.\"\n",
    ")\n",
    "\n",
    "# This is the list of tools you provide to the model.\n",
    "tools_For_Gemini = [price_function]\n",
    "\n",
    "# --- Helper Function for History ---\n",
    "def convert_history_to_gemini_format(history: List[Tuple[str, str]]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Converts Gradio's chat history format to Gemini's format.\n",
    "    Gradio: [(\"user message\", \"model message\"), ...]\n",
    "    Gemini: [{\"role\": \"user\", ...}, {\"role\": \"model\", ...}, ...]\n",
    "    \"\"\"\n",
    "    gemini_history = []\n",
    "    for user_msg, model_msg in history:\n",
    "        gemini_history.append({\"role\": \"user\", \"parts\": [{\"text\": user_msg}]})\n",
    "        gemini_history.append({\"role\": \"model\", \"parts\": [{\"text\": model_msg}]})\n",
    "    return gemini_history\n",
    "\n",
    "# --- Core Chat Logic ---\n",
    "def chat_Gemini_with_Tool(message: str, history: List[Tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    Handles the chat interaction with the Gemini model, including tool calls.\n",
    "    \"\"\"\n",
    "    # 1. Initialize the Gemini model\n",
    "    # We pass the tools directly to the model during initialization.\n",
    "    model_instance = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash\",\n",
    "        system_instruction=system_message,\n",
    "        tools=tools_For_Gemini\n",
    "    )\n",
    "\n",
    "    # 2. Convert and build the chat history for the API call\n",
    "    gemini_history = convert_history_to_gemini_format(history)\n",
    "    gemini_history.append({\"role\": \"user\", \"parts\": [{\"text\": message}]})\n",
    "\n",
    "    # 3. Call the model (without streaming) to check for tool calls\n",
    "    print(\"Sending request to Gemini...\")\n",
    "    response = model_instance.generate_content(\n",
    "        gemini_history,\n",
    "        tool_config={\"function_calling_config\": \"any\"}\n",
    "    )\n",
    "\n",
    "    # 4. Check if the model's response includes a tool call\n",
    "    candidate = response.candidates[0]\n",
    "    if candidate.finish_reason == 'TOOL_CALLS':\n",
    "        print(\"Gemini responded with a tool call.\")\n",
    "        # The model wants to call a function.\n",
    "        tool_call = candidate.content.parts[0].function_call\n",
    "        tool_name = tool_call.name\n",
    "\n",
    "        if tool_name == 'price_function':\n",
    "            # A. Extract arguments and call the actual Python function\n",
    "            args = {key: value for key, value in tool_call.args.items()}\n",
    "            tool_result = price_function(**args)\n",
    "\n",
    "            # B. Append the model's tool call and our tool's response to the history\n",
    "            gemini_history.append(candidate.content)  # Add model's request to history\n",
    "            gemini_history.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"parts\": [{\n",
    "                    \"function_response\": {\n",
    "                        \"name\": \"price_function\",\n",
    "                        \"response\": {\"result\": tool_result}\n",
    "                    }\n",
    "                }]\n",
    "            })\n",
    "\n",
    "            # C. Send the tool response back to the model to get a final answer\n",
    "            print(\"Sending tool result back to Gemini...\")\n",
    "            final_response = model_instance.generate_content(gemini_history)\n",
    "            return final_response.text # This will now be text\n",
    "        else:\n",
    "            return f\"Error: Model tried to call an unknown function: {tool_name}\"\n",
    "    else:\n",
    "        # 5. If no tool call, just return the model's text response\n",
    "        print(\"Gemini responded with text.\")\n",
    "        return response.text # This is safe because finish_reason was not TOOL_CALLS\n",
    "\n",
    "# --- Gradio UI ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Gradio interface...\")\n",
    "    iface = gr.ChatInterface(\n",
    "        fn=chat_Gemini_with_Tool,\n",
    "        title=\"Gemini Shopping Assistant\",\n",
    "        description=\"Ask me for the price of a shirt or jeans!\",\n",
    "        examples=[[\"How much is a shirt?\"], [\"What is the price of a pair of jeans?\"]]\n",
    "    )\n",
    "    iface.launch()\n",
    "    print(\"Gradio interface running.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
